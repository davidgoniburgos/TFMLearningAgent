# -*- coding: utf-8 -*-
"""TFM_Chatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ha_1uTLw3WhLKA4-4v95Gam1OcPuUOE0

https://planetachatbot.com/tutorial-chatbot-usando-nltk-keras/
https://github.com/hiteshmishra708/python-chatbot
"""

!pip install tensorflow 
!pip install keras 
!pip install pickle 
!pip install nltk

import nltk
nltk.download('punkt')
nltk.download('wordnet')
from nltk.stem import WordNetLemmatizer

import json
import pickle

import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Conv2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import SGD
#Added David
import re
#END Added

lemmatizer = WordNetLemmatizer()
words = []
classes = []
documents = []
ignore_words = ['?', '!']
data_file = open('intents.json').read()
intents = json.loads(data_file)

# Definición del personaje del chatbot para el entrenamiento
personaje = 'UNGAUNGA' 
# se remplaza el nombre definido por en la cadena <personaje>
#Convert to string and replace
obj_str = json.dumps(intents).replace('<personaje>', personaje)

#Get obj back with replacement
intents = json.loads(obj_str)
# se guardan los intents personalizados
with open('intents_personalizado.json', 'w') as outfile:
    json.dump(intents, outfile)
print(intents)

# intents: gruppi di conversazioni-tipo
# patterns: possibili interazioni dell'usuario
for intent in intents['intents']:
    for pattern in intent['patterns']:

        # tokenizzo ogni parola
        w = nltk.word_tokenize(pattern)
        words.extend(w)
        # aggiungo all'array documents
        documents.append((w, intent['tag']))
        print(w,intent['tag'])
        # adding classes to our class list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words]

pickle.dump(words, open('words.pkl','wb'))
pickle.dump(classes, open('classes.pkl','wb'))

# preparazione per l'addestramento della rete
training = []
output_empty = [0] * len(classes)
for doc in documents:
    # bag of words
    bag = []
    # lista di tokens
    pattern_words = doc[0]
    # lemmatizzazione dei token
    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in pattern_words]
    # se la parola matcha, inserisco 1, altriment 0
    for w in words:
        bag.append(1) if w in pattern_words else bag.append(0)

    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1

    training.append([bag, output_row])

training = np.array(training)
# creazione dei set di train e di test: X - patterns, Y - intents
train_x = list(training[:,0])
train_y = list(training[:,1])

# creazione del modello
model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))#128
model.add(Dropout(0.5))
model.add(Dense(16, activation='relu'))#64
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

#fitting and saving the model
hist = model.fit(np.array(train_x), np.array(train_y), epochs=300, batch_size=5, verbose=1)#epochs =300
model.save('chatbot_model.h5', hist)

print("model created")

import nltk, json, random, pickle
#Added David
from nltk import word_tokenize,pos_tag
nltk.download('averaged_perceptron_tagger')
nltk.download('maxent_ne_chunker')
nltk.download('words')
#END Added
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
import pickle
import numpy as np
from tensorflow.keras.models import load_model
model = load_model('chatbot_model.h5')
intents = json.loads(open('intents_personalizado.json').read())
words = pickle.load(open('words.pkl','rb'))
classes = pickle.load(open('classes.pkl','rb'))

# preprocessamento input user
def clean_up_sentence(sentence):
    sentence_words = nltk.word_tokenize(sentence)
    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]
    return sentence_words

# creazione bag of words
def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0]*len(words)
    for s in sentence_words:
        for i,w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print ("found in bag: %s" % w)
    return(np.array(bag))

def calcula_prediccion(sentence, model):
    p = bow(sentence, words,show_details=False)
    res = model.predict(np.array([p]))[0]
    #ERROR_THRESHOLD = 0.25
    ERROR_THRESHOLD = 0
    results = [[i,r] for i,r in enumerate(res) if r>ERROR_THRESHOLD]
    # sort by strength of probability
    results.sort(key=lambda x: x[1], reverse=True)
    return_list = []
    for r in results:
        print("intent:", classes[r[0]], " probability:", str(r[1]))
        return_list.append({"intent": classes[r[0]], "probability": str(r[1])})
    return return_list

def getRespuesta(ints, intents_json, entrada, name):
    tag = ints[0]['intent']
    list_of_intents = intents_json['intents']
    # Control de intents de ayuda nombre del personaje al principio
    if(re.match(rf"^{re.escape(personaje)}\b(?!\w)",str(entrada))):
      tag = 'ayuda'
    elif(tag == 'ayuda'):
      del ints[0]
      tag = ints[0]['intent']
    for i in list_of_intents:
        if(i['tag'] == tag):
          # Added David
          # Control del diálogo
          #get_DialogFlow(dialogFlow, tag)
          if (tag == 'minombre'):
            # Se extraen los NER
            tokens = word_tokenize(entrada)
            tag = pos_tag(tokens)
            ne_tree = nltk.ne_chunk(tag)
            print(ne_tree)
            # Extract PERSON tag for name
            if name == '':
              #name = re.findall('PERSON(.\S+)\/NNP', str(ne_tree))
              if (re.findall('\(S .+?\(PERSON(.\S+)\/NNP', str(ne_tree))):
                name = re.findall('\(S .+?\(PERSON(.\S+)\/NNP', str(ne_tree))
              print(name)
            result = random.choice(i['responses'])
            if name :
              result = re.sub('<name>', str(name[0]).strip(), result)
            else:
              result = re.sub('<name>', '', result)
            print(result)
            break   
            # END Added
          result = random.choice(i['responses'])
          if name :
              result = re.sub('<name>', str(name[0]).strip(), result)
          else:
              result = re.sub('<name>', '', result)
          break
    return result, name

def inizia(msg, name):
    ints = calcula_prediccion(msg, model)
    res , name = getRespuesta(ints, intents, msg, name)
    return res, name

def update_DialogFlow(dialogFlow, ints, res, action):
    dialogFlow.append[{'estado:'}]
    return True
def get_DialogFlow(dialogFlow, ints):
    return True

# Control del diálogo
dialogFlow = [{'estado':'inicio','intent':'','respuesta':'','intent_previo':''}]
# Introducción del reto
introduccion ='La vida de '+personaje+' es muy dura, y necesita tu ayuda para sobrevivir, ¿le puedes ayudar?'
usuario = ''
print('Para salir, escribe "salir"')
name = ''
print('¡Hola!, me llamo ', personaje,'!!')
print(introduccion)
intro=True
while usuario != 'salir':
  usuario = str(input(""))
  if (intro):
    ints = calcula_prediccion(usuario, model)
    if ints[0]['intent']=='afirmativo':
      res , name = getRespuesta(ints, intents, usuario, name)
      #dialogFlow[] 
    else:
      res = 'Vaya!, bueno nos vemos en otra ocasión.'
      usuario = 'salir'
    intro=False
  else:
    res, name  = inizia(usuario, name)
  print(personaje,':' + res)

"""https://www.youtube.com/watch?v=1lwddP0KUEg

https://www.machinecurve.com/index.php/2021/03/16/easy-chatbot-with-dialogpt-machine-learning-and-huggingface-transformers/
"""